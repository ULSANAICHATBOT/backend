from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import nest_asyncio
from pyngrok import conf, ngrok
import uvicorn

import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

# ëª¨ë¸ ë¡œë“œ
base_model = "morirokim/ulsan_ai_tour_model"

baseModel = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    # device_map={"": 0}
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

# pad_token ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# pipeline ìƒì„±
pipe = pipeline(
    "text-generation",
    model=baseModel,
    tokenizer=tokenizer,
    # device=0  # CUDA ì‚¬ìš©
)

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ í˜•ì‹ ì •ì˜
class Question(BaseModel):
    question: str

# LLaMA3 ì‘ë‹µ í•¨ìˆ˜
def extract_response_llama3(instruction, input_text="", system_message="ë„ˆëŠ” ìš¸ì‚° ê´€ê´‘ì§€ ì „ë¬¸ê°€ì•¼. í•­ìƒ ë‹µë³€ ë§ˆì§€ë§‰ì— 'ê°ì‚¬í•©ë‹ˆë‹¤' ë¼ê³  ë§í•´ì•¼í•´."):
    user_content = instruction
    if input_text:
        user_content += f"\n{input_text}"

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_content}
    ]

    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # ğŸ› ï¸ eos_tokenê³¼ eot_tokenì„ intë¡œ ë³€í™˜
    eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
    try:
        eot_token_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")
        if isinstance(eot_token_id, str):  # ì•ˆì „ì„± ì²´í¬
            eot_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
    except:
        eot_token_id = eos_token_id  # fallback

    outputs = pipe(
        prompt,
        max_new_tokens=256,
        eos_token_id=[eos_token_id, eot_token_id],
        do_sample=True,
        temperature=0.1,
        top_p=0.9,
        num_return_sequences=1,
        return_full_text=False
    )

    generated_text = outputs[0]["generated_text"].strip()
    return {"response": generated_text}

# ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat", status_code=200)
async def chat_with_bot(x: Question):
    print(x)
    question = x.question  # ì—¬ê¸°ì— .question ë¹ ì ¸ìˆì—ˆìŒ
    response = extract_response_llama3(question)
    print(response)
    return {"response": response}

# ngrok ì„¸íŒ…
conf.get_default().auth_token = os.getenv("NGROK_AUTHTOKEN")
nest_asyncio.apply()
ngrok_tunnel = ngrok.connect(8001)
print(f"ğŸŒ Public URL: {ngrok_tunnel.public_url}")

# ì„œë²„ ì‹¤í–‰
uvicorn.run(app, port=8001)